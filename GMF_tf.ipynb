{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "dataset = 'ml-1m'\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "num_factors = 8  # embedding size tmp\n",
    "regs = [0.0,0.0] # user, item embedding matrix regularizer parameter\n",
    "num_neg = 4      # negative sampling times (num_pos : num_neg)\n",
    "lr = 0.001       # learning rate\n",
    "topK = 10        # evaluation top item number \n",
    "evaluation_threads = 1\n",
    "model_out_file = 'Pretrain/%s_GMF_%d_%d.h5' %(dataset, num_factors, time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load train data \n",
    "### user, item, rating, time 으로 구성된 raw data로 부터\n",
    "### (user, item) value 1인 implicit feedback dictionary로 변환\n",
    "trn = pd.read_csv(path+dataset+\".train.rating\", sep=\"\\t\", names=['user','item','rating','time'])\n",
    "num_users=max(trn.user) + 1\n",
    "num_items=max(trn.item) + 1\n",
    "trn = trn[trn['rating']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load test data\n",
    "### testRatings는 [user, item] list\n",
    "tst = pd.read_csv(\"data/ml-1m.test.rating\", sep=\"\\t\", names=['user','item'], usecols = [0,1])\n",
    "testRatings = tst.values.tolist()\n",
    "### testNegatives는 [item_list]이고 index는 user와 동일\n",
    "neg = pd.read_csv(\"data/ml-1m.test.negative\", sep=\"_\", names=['neg_list'])\n",
    "testNegatives = neg.neg_list.map(lambda x: list(map(int, x.split('\\t')[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  item  rating       time\n",
      "0     0    32       4  978824330\n",
      "1     0    34       4  978824330\n",
      "2     0     4       5  978824291\n",
      "[[0, 25], [1, 133], [2, 207], [3, 208], [4, 222]]\n",
      "[1064, 174, 2791, 3373, 269, 2678, 1902, 3641, 1216, 915, 3672, 2803, 2344, 986, 3217, 2824, 2598, 464, 2340, 1952, 1855, 1353, 1547, 3487, 3293, 1541, 2414, 2728, 340, 1421, 1963, 2545, 972, 487, 3463, 2727, 1135, 3135, 128, 175, 2423, 1974, 2515, 3278, 3079, 1527, 2182, 1018, 2800, 1830, 1539, 617, 247, 3448, 1699, 1420, 2487, 198, 811, 1010, 1423, 2840, 1770, 881, 1913, 1803, 1734, 3326, 1617, 224, 3352, 1869, 1182, 1331, 336, 2517, 1721, 3512, 3656, 273, 1026, 1991, 2190, 998, 3386, 3369, 185, 2822, 864, 2854, 3067, 58, 2551, 2333, 2688, 3703, 1300, 1924, 3118]\n"
     ]
    }
   ],
   "source": [
    "print(trn[:3])\n",
    "print(testRatings[0:5])\n",
    "print(testNegatives[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Negative sampling and Make Input Data\n",
    "### user 별 item list 데이터를 만든다.\n",
    "trn_ = trn.groupby('user').item.agg(lambda x: list(x))\n",
    "### user 별 item list에서 test set의 item이 아닌 item 중에서 negative item sampling을 수행한다.\n",
    "item_input = []\n",
    "labels = []\n",
    "for u, (l, n, t) in enumerate(zip(trn_, testNegatives, testRatings)):\n",
    "    ## l은 train item, n은 test neg item, t[1]은 tst item으로\n",
    "    ## train & test set에 포함되지 않는 item 중 num_neg 배수만큼 추출한다.\n",
    "    ## 즉, 1번 user가 100개의 item이 있다면 neg item은 400개를 추출 하는 것이다.\n",
    "    neg_set = list(set(range(num_items)) - set(l + n + [t[1]]))\n",
    "    neg_set = random.sample(neg_set, min(len(l)*num_neg, len(neg_set)))\n",
    "    ## train positive, negative item list를 구성하고 그에 맞게 label list를 구성한다.\n",
    "    item_input.append(l + neg_set)\n",
    "    labels.append([1]*len(l) + [0]*len(neg_set))\n",
    "### item_input에 맞에 user_input을 구성한다. \n",
    "user_input = [[i]*len(l) for i, l in enumerate(item_input)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2d list to 1d list\n",
    "user_input = [j for sub in user_input for j in sub]\n",
    "item_input = [j for sub in item_input for j in sub]\n",
    "labels = [j for sub in labels for j in sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## placeholder\n",
    "user_input_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='user_input')\n",
    "item_input_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='itme_input')\n",
    "labels_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "## user and item embedding matrix\n",
    "with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n",
    "    Embedding_User = tf.get_variable(name=\"embedding_users\", shape=[num_users, num_factors],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    regularizer=tf.contrib.layers.l2_regularizer(regs[0]))\n",
    "    Embedding_Item = tf.get_variable(name=\"embedding_items\", shape=[num_items, num_factors],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    regularizer=tf.contrib.layers.l2_regularizer(regs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prediction for user, item set\n",
    "### embedding lookup for user and item set\n",
    "user_latent = tf.nn.embedding_lookup(Embedding_User, user_input_ph)\n",
    "item_latent = tf.nn.embedding_lookup(Embedding_Item, item_input_ph)\n",
    "### user X item: shape [None, 8]\n",
    "predict_vector = tf.multiply(user_latent, item_latent)\n",
    "### weighted sum: weight가 모두 1이라면 user, item의 내적과 동일하지만, 각각의 weight를 다르게 하여 합하였다.\n",
    "### shape: [None, 1]\n",
    "prediction = tf.contrib.layers.fully_connected(predict_vector, 1, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loss and optimizer\n",
    "### label shape을 prediction과 동일하게 [None, 1]로 변경해주고, loss(cost)를 계산한다.\n",
    "loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(tf.reshape(labels_ph, [-1,1]), tf.float32), \n",
    "            logits=prediction)\n",
    ")\n",
    "## Adam Optimizer \n",
    "with tf.variable_scope(\"optimizer\", reuse=tf.AUTO_REUSE):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### test set의 구성은 user별 positive item 1개와 negetive item 99개로 구성되어 있다.\n",
    "### evaluaton process\n",
    "### 1. 유저별 neg, pos 100개의 item에 대하여 model에 input하여 prediction을 계산한다.\n",
    "### 2. 100개의 prediction 값 중 내림차순으로 상위 topK(10개) item을 순서를 유지하여 추출한다.\n",
    "### 3. HitRatio는 10개 중 positive item이 있으면 1 없으면 0으로 값을 return한다.\n",
    "### 4. NDCG는 positive item의 순위를 고려하여 계산한것이다. 즉, rank 1이라면 값은 1이 되고,\n",
    "###    rank가 2, 3... 뒤로 갈 수록 log비율로 감소시킨다. 10개 중 없으면 0을 리턴함.\n",
    "### 5. user별 HitRatio와 NDCG 값을 list로 구성하고 평균을 구하면 최종 평가 지표가 된다. \n",
    "def evaluation(sess, testRatings, testNegatives, K):\n",
    "    def getHitRatio(ranklist, gtItem):\n",
    "        for item in ranklist:\n",
    "            if item == gtItem:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def getNDCG(ranklist, gtItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == gtItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "    \n",
    "    hits, ndcgs = [],[]\n",
    "    for i in range(len(testRatings)):\n",
    "        rating = testRatings[i]\n",
    "        items = testNegatives[i]\n",
    "        u = rating[0]\n",
    "        gtItem = rating[1]\n",
    "        items.append(gtItem)\n",
    "        \n",
    "        # Get prediction scores\n",
    "        map_item_score = {}\n",
    "        users = np.full(len(items), u, dtype = 'int32')\n",
    "        predictions = sess.run(prediction, \n",
    "                               feed_dict={user_input_ph: users, item_input_ph: items})\n",
    "\n",
    "        for i in range(len(items)):\n",
    "            item = items[i]\n",
    "            map_item_score[item] = predictions[i]\n",
    "        items.pop()\n",
    "        \n",
    "        # Evaluate top rank list\n",
    "        ranklist = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        \n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    \n",
    "    return(hits, ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## session start\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation\n",
    "hits, ndcgs = evaluation(sess, testRatings, testNegatives, topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.0978, NDCG = 0.0453\n"
     ]
    }
   ],
   "source": [
    "## initial HitRatio and NDCG\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f' % (hr, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 batchs: 0 \n",
      " mean_loss: 0.69314647\n",
      "epochs: 0 batchs: 1000 \n",
      " mean_loss: 0.59377944\n",
      "epochs: 0 batchs: 2000 \n",
      " mean_loss: 0.50942606\n",
      "epochs: 0 batchs: 3000 \n",
      " mean_loss: 0.46495125\n",
      "epochs: 0 batchs: 4000 \n",
      " mean_loss: 0.44106224\n",
      "epochs: 0 batchs: 5000 \n",
      " mean_loss: 0.42589998\n",
      "epochs: 0 batchs: 6000 \n",
      " mean_loss: 0.41549882\n",
      "epochs: 0 batchs: 7000 \n",
      " mean_loss: 0.40771037\n",
      "epochs: 0 batchs: 8000 \n",
      " mean_loss: 0.40142468\n",
      "epochs: 0 batchs: 9000 \n",
      " mean_loss: 0.3963859\n",
      "epochs: 0 batchs: 10000 \n",
      " mean_loss: 0.39231136\n",
      "epochs: 0 batchs: 11000 \n",
      " mean_loss: 0.3888082\n",
      "epochs: 0 batchs: 12000 \n",
      " mean_loss: 0.38550532\n",
      "epochs: 0 batchs: 13000 \n",
      " mean_loss: 0.38278028\n",
      "epochs: 0 batchs: 14000 \n",
      " mean_loss: 0.3801882\n",
      "epochs: 0 batchs: 15000 \n",
      " mean_loss: 0.37780902\n",
      "epochs: 0 batchs: 16000 \n",
      " mean_loss: 0.37551004\n",
      "epochs: 0 batchs: 17000 \n",
      " mean_loss: 0.37334803\n",
      "epochs: 0 batchs: 18000 \n",
      " mean_loss: 0.37112635\n",
      "eopch 0 - HR = 0.5119, NDCG = 0.2858\n",
      "epochs: 1 batchs: 0 \n",
      " mean_loss: 0.2782477\n",
      "epochs: 1 batchs: 1000 \n",
      " mean_loss: 0.32368562\n",
      "epochs: 1 batchs: 2000 \n",
      " mean_loss: 0.32249263\n",
      "epochs: 1 batchs: 3000 \n",
      " mean_loss: 0.3212877\n",
      "epochs: 1 batchs: 4000 \n",
      " mean_loss: 0.320055\n",
      "epochs: 1 batchs: 5000 \n",
      " mean_loss: 0.3186521\n",
      "epochs: 1 batchs: 6000 \n",
      " mean_loss: 0.3177499\n",
      "epochs: 1 batchs: 7000 \n",
      " mean_loss: 0.316665\n",
      "epochs: 1 batchs: 8000 \n",
      " mean_loss: 0.31582803\n",
      "epochs: 1 batchs: 9000 \n",
      " mean_loss: 0.31489328\n",
      "epochs: 1 batchs: 10000 \n",
      " mean_loss: 0.31407338\n",
      "epochs: 1 batchs: 11000 \n",
      " mean_loss: 0.31320128\n",
      "epochs: 1 batchs: 12000 \n",
      " mean_loss: 0.31255436\n",
      "epochs: 1 batchs: 13000 \n",
      " mean_loss: 0.31179938\n",
      "epochs: 1 batchs: 14000 \n",
      " mean_loss: 0.3111308\n",
      "epochs: 1 batchs: 15000 \n",
      " mean_loss: 0.31055757\n",
      "epochs: 1 batchs: 16000 \n",
      " mean_loss: 0.3100304\n",
      "epochs: 1 batchs: 17000 \n",
      " mean_loss: 0.30937767\n",
      "epochs: 1 batchs: 18000 \n",
      " mean_loss: 0.30881923\n",
      "eopch 1 - HR = 0.5768, NDCG = 0.3253\n",
      "epochs: 2 batchs: 0 \n",
      " mean_loss: 0.3256533\n",
      "epochs: 2 batchs: 1000 \n",
      " mean_loss: 0.2920804\n",
      "epochs: 2 batchs: 2000 \n",
      " mean_loss: 0.29251218\n",
      "epochs: 2 batchs: 3000 \n",
      " mean_loss: 0.292701\n",
      "epochs: 2 batchs: 4000 \n",
      " mean_loss: 0.29287437\n",
      "epochs: 2 batchs: 5000 \n",
      " mean_loss: 0.2928826\n",
      "epochs: 2 batchs: 6000 \n",
      " mean_loss: 0.29305735\n",
      "epochs: 2 batchs: 7000 \n",
      " mean_loss: 0.29289907\n",
      "epochs: 2 batchs: 8000 \n",
      " mean_loss: 0.29315144\n",
      "epochs: 2 batchs: 9000 \n",
      " mean_loss: 0.2932311\n",
      "epochs: 2 batchs: 10000 \n",
      " mean_loss: 0.29317972\n",
      "epochs: 2 batchs: 11000 \n",
      " mean_loss: 0.29323536\n",
      "epochs: 2 batchs: 12000 \n",
      " mean_loss: 0.29336837\n",
      "epochs: 2 batchs: 13000 \n",
      " mean_loss: 0.29333574\n",
      "epochs: 2 batchs: 14000 \n",
      " mean_loss: 0.29318756\n",
      "epochs: 2 batchs: 15000 \n",
      " mean_loss: 0.29313162\n",
      "epochs: 2 batchs: 16000 \n",
      " mean_loss: 0.2930069\n",
      "epochs: 2 batchs: 17000 \n",
      " mean_loss: 0.29294518\n",
      "epochs: 2 batchs: 18000 \n",
      " mean_loss: 0.29288375\n",
      "eopch 2 - HR = 0.5967, NDCG = 0.3381\n",
      "epochs: 3 batchs: 0 \n",
      " mean_loss: 0.22751662\n",
      "epochs: 3 batchs: 1000 \n",
      " mean_loss: 0.285102\n",
      "epochs: 3 batchs: 2000 \n",
      " mean_loss: 0.28568262\n",
      "epochs: 3 batchs: 3000 \n",
      " mean_loss: 0.28613502\n",
      "epochs: 3 batchs: 4000 \n",
      " mean_loss: 0.28594804\n",
      "epochs: 3 batchs: 5000 \n",
      " mean_loss: 0.28630832\n",
      "epochs: 3 batchs: 6000 \n",
      " mean_loss: 0.2863702\n",
      "epochs: 3 batchs: 7000 \n",
      " mean_loss: 0.2862976\n",
      "epochs: 3 batchs: 8000 \n",
      " mean_loss: 0.28661376\n",
      "epochs: 3 batchs: 9000 \n",
      " mean_loss: 0.2866826\n",
      "epochs: 3 batchs: 10000 \n",
      " mean_loss: 0.28655976\n",
      "epochs: 3 batchs: 11000 \n",
      " mean_loss: 0.28652367\n",
      "epochs: 3 batchs: 12000 \n",
      " mean_loss: 0.28651318\n",
      "epochs: 3 batchs: 13000 \n",
      " mean_loss: 0.2864428\n",
      "epochs: 3 batchs: 14000 \n",
      " mean_loss: 0.28643933\n",
      "epochs: 3 batchs: 15000 \n",
      " mean_loss: 0.2863481\n",
      "epochs: 3 batchs: 16000 \n",
      " mean_loss: 0.28635648\n",
      "epochs: 3 batchs: 17000 \n",
      " mean_loss: 0.28639257\n",
      "epochs: 3 batchs: 18000 \n",
      " mean_loss: 0.2863423\n",
      "eopch 3 - HR = 0.6031, NDCG = 0.3425\n",
      "epochs: 4 batchs: 0 \n",
      " mean_loss: 0.27869287\n",
      "epochs: 4 batchs: 1000 \n",
      " mean_loss: 0.27815628\n",
      "epochs: 4 batchs: 2000 \n",
      " mean_loss: 0.27918696\n",
      "epochs: 4 batchs: 3000 \n",
      " mean_loss: 0.27956674\n",
      "epochs: 4 batchs: 4000 \n",
      " mean_loss: 0.27993858\n",
      "epochs: 4 batchs: 5000 \n",
      " mean_loss: 0.2801391\n",
      "epochs: 4 batchs: 6000 \n",
      " mean_loss: 0.28014895\n",
      "epochs: 4 batchs: 7000 \n",
      " mean_loss: 0.28049713\n",
      "epochs: 4 batchs: 8000 \n",
      " mean_loss: 0.28044212\n",
      "epochs: 4 batchs: 9000 \n",
      " mean_loss: 0.28044844\n",
      "epochs: 4 batchs: 10000 \n",
      " mean_loss: 0.28061867\n",
      "epochs: 4 batchs: 11000 \n",
      " mean_loss: 0.28098288\n",
      "epochs: 4 batchs: 12000 \n",
      " mean_loss: 0.28115594\n",
      "epochs: 4 batchs: 13000 \n",
      " mean_loss: 0.28132826\n",
      "epochs: 4 batchs: 14000 \n",
      " mean_loss: 0.28128913\n",
      "epochs: 4 batchs: 15000 \n",
      " mean_loss: 0.28139582\n",
      "epochs: 4 batchs: 16000 \n",
      " mean_loss: 0.281328\n",
      "epochs: 4 batchs: 17000 \n",
      " mean_loss: 0.28134236\n",
      "epochs: 4 batchs: 18000 \n",
      " mean_loss: 0.28142288\n",
      "eopch 4 - HR = 0.6093, NDCG = 0.3492\n",
      "epochs: 5 batchs: 0 \n",
      " mean_loss: 0.30972022\n",
      "epochs: 5 batchs: 1000 \n",
      " mean_loss: 0.27581805\n",
      "epochs: 5 batchs: 2000 \n",
      " mean_loss: 0.27636153\n",
      "epochs: 5 batchs: 3000 \n",
      " mean_loss: 0.2759247\n",
      "epochs: 5 batchs: 4000 \n",
      " mean_loss: 0.27616385\n",
      "epochs: 5 batchs: 5000 \n",
      " mean_loss: 0.27643618\n",
      "epochs: 5 batchs: 6000 \n",
      " mean_loss: 0.27630943\n",
      "epochs: 5 batchs: 7000 \n",
      " mean_loss: 0.27642035\n",
      "epochs: 5 batchs: 8000 \n",
      " mean_loss: 0.2764101\n",
      "epochs: 5 batchs: 9000 \n",
      " mean_loss: 0.27650434\n",
      "epochs: 5 batchs: 10000 \n",
      " mean_loss: 0.27686128\n",
      "epochs: 5 batchs: 11000 \n",
      " mean_loss: 0.27706698\n",
      "epochs: 5 batchs: 12000 \n",
      " mean_loss: 0.27706856\n",
      "epochs: 5 batchs: 13000 \n",
      " mean_loss: 0.27723163\n",
      "epochs: 5 batchs: 14000 \n",
      " mean_loss: 0.2774823\n",
      "epochs: 5 batchs: 15000 \n",
      " mean_loss: 0.2775652\n",
      "epochs: 5 batchs: 16000 \n",
      " mean_loss: 0.2776231\n",
      "epochs: 5 batchs: 17000 \n",
      " mean_loss: 0.27776134\n",
      "epochs: 5 batchs: 18000 \n",
      " mean_loss: 0.27803427\n",
      "eopch 5 - HR = 0.6202, NDCG = 0.3569\n",
      "epochs: 6 batchs: 0 \n",
      " mean_loss: 0.25877228\n",
      "epochs: 6 batchs: 1000 \n",
      " mean_loss: 0.2714042\n",
      "epochs: 6 batchs: 2000 \n",
      " mean_loss: 0.27055547\n",
      "epochs: 6 batchs: 3000 \n",
      " mean_loss: 0.27112976\n",
      "epochs: 6 batchs: 4000 \n",
      " mean_loss: 0.27180332\n",
      "epochs: 6 batchs: 5000 \n",
      " mean_loss: 0.2725634\n",
      "epochs: 6 batchs: 6000 \n",
      " mean_loss: 0.27286533\n",
      "epochs: 6 batchs: 7000 \n",
      " mean_loss: 0.27322665\n",
      "epochs: 6 batchs: 8000 \n",
      " mean_loss: 0.27351236\n",
      "epochs: 6 batchs: 9000 \n",
      " mean_loss: 0.27367938\n",
      "epochs: 6 batchs: 10000 \n",
      " mean_loss: 0.273918\n",
      "epochs: 6 batchs: 11000 \n",
      " mean_loss: 0.27417803\n",
      "epochs: 6 batchs: 12000 \n",
      " mean_loss: 0.27439097\n",
      "epochs: 6 batchs: 13000 \n",
      " mean_loss: 0.27455243\n",
      "epochs: 6 batchs: 14000 \n",
      " mean_loss: 0.27480704\n",
      "epochs: 6 batchs: 15000 \n",
      " mean_loss: 0.27481207\n",
      "epochs: 6 batchs: 16000 \n",
      " mean_loss: 0.27502286\n",
      "epochs: 6 batchs: 17000 \n",
      " mean_loss: 0.2751079\n",
      "epochs: 6 batchs: 18000 \n",
      " mean_loss: 0.2752381\n",
      "eopch 6 - HR = 0.6184, NDCG = 0.3576\n"
     ]
    }
   ],
   "source": [
    "## Train model\n",
    "### early stopping을 위한 값 세팅\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    ### data shuffle\n",
    "    user_input, item_input, labels = shuffle(\n",
    "        user_input, item_input, labels\n",
    "    )\n",
    "    ### training\n",
    "    batch_len = len(user_input) // batch_size\n",
    "    batch_loss = list()\n",
    "    for i in range(batch_len):\n",
    "        start = i*batch_size\n",
    "        user_input_bc = user_input[start : start+batch_size]\n",
    "        item_input_bc = item_input[start : start+batch_size]\n",
    "        labels_bc = labels[start : start+batch_size]\n",
    "    \n",
    "        _, l = sess.run([train_op, loss], feed_dict={\n",
    "            user_input_ph: user_input_bc,\n",
    "            item_input_ph: item_input_bc,\n",
    "            labels_ph: labels_bc\n",
    "        })\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        if i % 1000==0:\n",
    "            print(\"epochs:\", epoch, \"batchs:\", i, \"\\n\", \"mean_loss:\", \n",
    "                  np.array(batch_loss).mean())\n",
    "    \n",
    "    ### evaluation\n",
    "    hits, ndcgs = evaluation(sess, testRatings, testNegatives, topK)\n",
    "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print('eopch', epoch, '- HR = %.4f, NDCG = %.4f' % (hr, ndcg))\n",
    "    ### HitRatio가 이전 값보다 작아지면 종료한다.\n",
    "    if hr > best_hr:\n",
    "        best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
