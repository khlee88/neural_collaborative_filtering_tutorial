{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "dataset = 'ml-1m'\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "layers = [128,64,64,32]\n",
    "reg_layers = [0.,0.,0.,0.]\n",
    "num_neg = 4\n",
    "lr = 0.001\n",
    "patience = 0     \n",
    "max_patience = 5\n",
    "topK = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load train data \n",
    "### user, item, rating, time 으로 구성된 raw data로 부터\n",
    "### (user, item) value 1인 implicit feedback dictionary로 변환\n",
    "trn = pd.read_csv(path+dataset+\".train.rating\", sep=\"\\t\", names=['user','item','rating','time'])\n",
    "num_users=max(trn.user) + 1\n",
    "num_items=max(trn.item) + 1\n",
    "trn = trn[trn['rating']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load test data\n",
    "### testRatings는 [user, item] list\n",
    "tst = pd.read_csv(\"data/ml-1m.test.rating\", sep=\"\\t\", names=['user','item'], usecols = [0,1])\n",
    "testRatings = tst.values.tolist()\n",
    "### testNegatives는 [item_list]이고 index는 user와 동일\n",
    "neg = pd.read_csv(\"data/ml-1m.test.negative\", sep=\"_\", names=['neg_list'])\n",
    "testNegatives = neg.neg_list.map(lambda x: list(map(int, x.split('\\t')[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  item  rating       time\n",
      "0     0    32       4  978824330\n",
      "1     0    34       4  978824330\n",
      "2     0     4       5  978824291\n",
      "[[0, 25], [1, 133], [2, 207], [3, 208], [4, 222]]\n",
      "[1064, 174, 2791, 3373, 269, 2678, 1902, 3641, 1216, 915, 3672, 2803, 2344, 986, 3217, 2824, 2598, 464, 2340, 1952, 1855, 1353, 1547, 3487, 3293, 1541, 2414, 2728, 340, 1421, 1963, 2545, 972, 487, 3463, 2727, 1135, 3135, 128, 175, 2423, 1974, 2515, 3278, 3079, 1527, 2182, 1018, 2800, 1830, 1539, 617, 247, 3448, 1699, 1420, 2487, 198, 811, 1010, 1423, 2840, 1770, 881, 1913, 1803, 1734, 3326, 1617, 224, 3352, 1869, 1182, 1331, 336, 2517, 1721, 3512, 3656, 273, 1026, 1991, 2190, 998, 3386, 3369, 185, 2822, 864, 2854, 3067, 58, 2551, 2333, 2688, 3703, 1300, 1924, 3118]\n"
     ]
    }
   ],
   "source": [
    "print(trn[:3])\n",
    "print(testRatings[0:5])\n",
    "print(testNegatives[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train Negative sampling and Make Input Data\n",
    "### user 별 item list 데이터를 만든다.\n",
    "trn_ = trn.groupby('user').item.agg(lambda x: list(x))\n",
    "### user 별 item list에서 test set의 item이 아닌 item 중에서 negative item sampling을 수행한다.\n",
    "item_input = []\n",
    "labels = []\n",
    "for u, (l, n, t) in enumerate(zip(trn_, testNegatives, testRatings)):\n",
    "    ## l은 train item, n은 test neg item, t[1]은 tst item으로\n",
    "    ## train & test set에 포함되지 않는 item 중 num_neg 배수만큼 추출한다.\n",
    "    ## 즉, 1번 user가 100개의 item이 있다면 neg item은 400개를 추출 하는 것이다.\n",
    "    neg_set = list(set(range(num_items)) - set(l + n + [t[1]]))\n",
    "    neg_set = random.sample(neg_set, min(len(l)*num_neg, len(neg_set)))\n",
    "    ## train positive, negative item list를 구성하고 그에 맞게 label list를 구성한다.\n",
    "    item_input.append(l + neg_set)\n",
    "    labels.append([1]*len(l) + [0]*len(neg_set))\n",
    "### item_input에 맞에 user_input을 구성한다. \n",
    "user_input = [[i]*len(l) for i, l in enumerate(item_input)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2d list to 1d list\n",
    "user_input = [j for sub in user_input for j in sub]\n",
    "item_input = [j for sub in item_input for j in sub]\n",
    "labels = [j for sub in labels for j in sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## placeholder\n",
    "user_input_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='user_input')\n",
    "item_input_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='itme_input')\n",
    "labels_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "## user and item embedding matrix\n",
    "### 첫번째 layer 노드 수의 1/2씩을 user, item의 embedding size로 생성\n",
    "with tf.variable_scope(\"mlp_embedding\", reuse=tf.AUTO_REUSE):\n",
    "    Embedding_User = tf.get_variable(name=\"embedding_users\", shape=[num_users, layers[0]/2],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    regularizer=tf.contrib.layers.l2_regularizer(reg_layers[0]))\n",
    "    Embedding_Item = tf.get_variable(name=\"embedding_items\", shape=[num_items, layers[0]/2],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    regularizer=tf.contrib.layers.l2_regularizer(reg_layers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "## prediction for user, item set\n",
    "### user, item set에 대한 embedding lookup 후 bind 한다.\n",
    "### user shape: [None, 64], item shape: [None, 64], hidden shape: [None, 128]\n",
    "user_latent = tf.nn.embedding_lookup(Embedding_User, user_input_ph)\n",
    "item_latent = tf.nn.embedding_lookup(Embedding_Item, item_input_ph)\n",
    "hidden = tf.concat([user_latent, item_latent], 1) \n",
    "### feed forward \n",
    "num_layer = len(layers)\n",
    "for idx in range(1, num_layer):\n",
    "    hidden = tf.contrib.layers.fully_connected(\n",
    "        hidden, layers[idx], activation_fn=tf.nn.relu,\n",
    "        weights_regularizer=tf.contrib.layers.l2_regularizer(reg_layers[idx])\n",
    "    )\n",
    "### final prediction layer: shape [None, 1]\n",
    "prediction = tf.contrib.layers.fully_connected(hidden, 1, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loss and optimizer\n",
    "### label shape을 prediction과 동일하게 [None, 1]로 변경해주고, loss(cost)를 계산한다.\n",
    "loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(tf.reshape(labels_ph, [-1,1]), tf.float32), \n",
    "            logits=prediction)\n",
    ")\n",
    "## Adam Optimizer \n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### test set의 구성은 user별 positive item 1개와 negetive item 99개로 구성되어 있다.\n",
    "### evaluaton process\n",
    "### 1. 유저별 neg, pos 100개의 item에 대하여 model에 input하여 prediction을 계산한다.\n",
    "### 2. 100개의 prediction 값 중 내림차순으로 상위 topK(10개) item을 순서를 유지하여 추출한다.\n",
    "### 3. HitRatio는 10개 중 positive item이 있으면 1 없으면 0으로 값을 return한다.\n",
    "### 4. NDCG는 positive item의 순위를 고려하여 계산한것이다. 즉, rank 1이라면 값은 1이 되고,\n",
    "###    rank가 2, 3... 뒤로 갈 수록 log비율로 감소시킨다. 10개 중 없으면 0을 리턴함.\n",
    "### 5. user별 HitRatio와 NDCG 값을 list로 구성하고 평균을 구하면 최종 평가 지표가 된다. \n",
    "def evaluation(sess, testRatings, testNegatives, K):\n",
    "    def getHitRatio(ranklist, gtItem):\n",
    "        for item in ranklist:\n",
    "            if item == gtItem:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def getNDCG(ranklist, gtItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == gtItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "    \n",
    "    hits, ndcgs = [],[]\n",
    "    for i in range(len(testRatings)):\n",
    "        rating = testRatings[i]\n",
    "        items = testNegatives[i]\n",
    "        u = rating[0]\n",
    "        gtItem = rating[1]\n",
    "        items.append(gtItem)\n",
    "        \n",
    "        # Get prediction scores\n",
    "        map_item_score = {}\n",
    "        users = np.full(len(items), u, dtype = 'int32')\n",
    "        predictions = sess.run(prediction, \n",
    "                               feed_dict={user_input_ph: users, item_input_ph: items})\n",
    "\n",
    "        for i in range(len(items)):\n",
    "            item = items[i]\n",
    "            map_item_score[item] = predictions[i]\n",
    "        items.pop()\n",
    "        \n",
    "        # Evaluate top rank list\n",
    "        ranklist = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        \n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    \n",
    "    return(hits, ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## session start\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## evaluation\n",
    "hits, ndcgs = evaluation(sess, testRatings, testNegatives, topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.0863, NDCG = 0.0386\n"
     ]
    }
   ],
   "source": [
    "## initial HitRatio and NDCG\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f' % (hr, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 batchs: 0 \n",
      " mean_loss: 0.6924999\n",
      "epochs: 0 batchs: 1000 \n",
      " mean_loss: 0.39549094\n",
      "epochs: 0 batchs: 2000 \n",
      " mean_loss: 0.3823371\n",
      "epochs: 0 batchs: 3000 \n",
      " mean_loss: 0.3772449\n",
      "epochs: 0 batchs: 4000 \n",
      " mean_loss: 0.3741326\n",
      "epochs: 0 batchs: 5000 \n",
      " mean_loss: 0.37182364\n",
      "epochs: 0 batchs: 6000 \n",
      " mean_loss: 0.36949572\n",
      "epochs: 0 batchs: 7000 \n",
      " mean_loss: 0.36708027\n",
      "epochs: 0 batchs: 8000 \n",
      " mean_loss: 0.36436498\n",
      "epochs: 0 batchs: 9000 \n",
      " mean_loss: 0.3619168\n",
      "epochs: 0 batchs: 10000 \n",
      " mean_loss: 0.35962448\n",
      "epochs: 0 batchs: 11000 \n",
      " mean_loss: 0.35753492\n",
      "epochs: 0 batchs: 12000 \n",
      " mean_loss: 0.3551565\n",
      "epochs: 0 batchs: 13000 \n",
      " mean_loss: 0.35292223\n",
      "epochs: 0 batchs: 14000 \n",
      " mean_loss: 0.3507661\n",
      "epochs: 0 batchs: 15000 \n",
      " mean_loss: 0.34882343\n",
      "epochs: 0 batchs: 16000 \n",
      " mean_loss: 0.3468434\n",
      "epochs: 0 batchs: 17000 \n",
      " mean_loss: 0.34489065\n",
      "epochs: 0 batchs: 18000 \n",
      " mean_loss: 0.34317765\n",
      "eopch 0 - HR = 0.5583, NDCG = 0.3109 elapsed time: 62.53\n",
      "epochs: 1 batchs: 0 \n",
      " mean_loss: 0.28337398\n",
      "epochs: 1 batchs: 1000 \n",
      " mean_loss: 0.3030587\n",
      "epochs: 1 batchs: 2000 \n",
      " mean_loss: 0.30206242\n",
      "epochs: 1 batchs: 3000 \n",
      " mean_loss: 0.3015304\n",
      "epochs: 1 batchs: 4000 \n",
      " mean_loss: 0.3009191\n",
      "epochs: 1 batchs: 5000 \n",
      " mean_loss: 0.3010038\n",
      "epochs: 1 batchs: 6000 \n",
      " mean_loss: 0.30053097\n",
      "epochs: 1 batchs: 7000 \n",
      " mean_loss: 0.30002117\n",
      "epochs: 1 batchs: 8000 \n",
      " mean_loss: 0.29942426\n",
      "epochs: 1 batchs: 9000 \n",
      " mean_loss: 0.29874238\n",
      "epochs: 1 batchs: 10000 \n",
      " mean_loss: 0.29814598\n",
      "epochs: 1 batchs: 11000 \n",
      " mean_loss: 0.29748467\n",
      "epochs: 1 batchs: 12000 \n",
      " mean_loss: 0.29691264\n",
      "epochs: 1 batchs: 13000 \n",
      " mean_loss: 0.2964245\n",
      "epochs: 1 batchs: 14000 \n",
      " mean_loss: 0.29602247\n",
      "epochs: 1 batchs: 15000 \n",
      " mean_loss: 0.29562137\n",
      "epochs: 1 batchs: 16000 \n",
      " mean_loss: 0.2950599\n",
      "epochs: 1 batchs: 17000 \n",
      " mean_loss: 0.29462227\n",
      "epochs: 1 batchs: 18000 \n",
      " mean_loss: 0.29414585\n",
      "eopch 1 - HR = 0.6111, NDCG = 0.3467 elapsed time: 61.62\n",
      "epochs: 2 batchs: 0 \n",
      " mean_loss: 0.2575499\n",
      "epochs: 2 batchs: 1000 \n",
      " mean_loss: 0.2755085\n",
      "epochs: 2 batchs: 2000 \n",
      " mean_loss: 0.27626595\n",
      "epochs: 2 batchs: 3000 \n",
      " mean_loss: 0.27643433\n",
      "epochs: 2 batchs: 4000 \n",
      " mean_loss: 0.27639788\n",
      "epochs: 2 batchs: 5000 \n",
      " mean_loss: 0.27622586\n",
      "epochs: 2 batchs: 6000 \n",
      " mean_loss: 0.27597058\n",
      "epochs: 2 batchs: 7000 \n",
      " mean_loss: 0.2762656\n",
      "epochs: 2 batchs: 8000 \n",
      " mean_loss: 0.27641606\n",
      "epochs: 2 batchs: 9000 \n",
      " mean_loss: 0.27638125\n",
      "epochs: 2 batchs: 10000 \n",
      " mean_loss: 0.2763427\n",
      "epochs: 2 batchs: 11000 \n",
      " mean_loss: 0.2763895\n",
      "epochs: 2 batchs: 12000 \n",
      " mean_loss: 0.2763513\n",
      "epochs: 2 batchs: 13000 \n",
      " mean_loss: 0.27630123\n",
      "epochs: 2 batchs: 14000 \n",
      " mean_loss: 0.2762795\n",
      "epochs: 2 batchs: 15000 \n",
      " mean_loss: 0.2761691\n",
      "epochs: 2 batchs: 16000 \n",
      " mean_loss: 0.2761086\n",
      "epochs: 2 batchs: 17000 \n",
      " mean_loss: 0.27611616\n",
      "epochs: 2 batchs: 18000 \n",
      " mean_loss: 0.27596873\n",
      "eopch 2 - HR = 0.6344, NDCG = 0.3669 elapsed time: 59.77\n",
      "epochs: 3 batchs: 0 \n",
      " mean_loss: 0.3103143\n",
      "epochs: 3 batchs: 1000 \n",
      " mean_loss: 0.25872588\n",
      "epochs: 3 batchs: 2000 \n",
      " mean_loss: 0.2602046\n",
      "epochs: 3 batchs: 3000 \n",
      " mean_loss: 0.26137903\n",
      "epochs: 3 batchs: 4000 \n",
      " mean_loss: 0.26187864\n",
      "epochs: 3 batchs: 5000 \n",
      " mean_loss: 0.26246175\n",
      "epochs: 3 batchs: 6000 \n",
      " mean_loss: 0.2625697\n",
      "epochs: 3 batchs: 7000 \n",
      " mean_loss: 0.26285383\n",
      "epochs: 3 batchs: 8000 \n",
      " mean_loss: 0.26316223\n",
      "epochs: 3 batchs: 9000 \n",
      " mean_loss: 0.26366943\n",
      "epochs: 3 batchs: 10000 \n",
      " mean_loss: 0.2639868\n",
      "epochs: 3 batchs: 11000 \n",
      " mean_loss: 0.26414123\n",
      "epochs: 3 batchs: 12000 \n",
      " mean_loss: 0.2642557\n",
      "epochs: 3 batchs: 13000 \n",
      " mean_loss: 0.2643272\n",
      "epochs: 3 batchs: 14000 \n",
      " mean_loss: 0.26449004\n",
      "epochs: 3 batchs: 15000 \n",
      " mean_loss: 0.26453504\n",
      "epochs: 3 batchs: 16000 \n",
      " mean_loss: 0.26450807\n",
      "epochs: 3 batchs: 17000 \n",
      " mean_loss: 0.26454896\n",
      "epochs: 3 batchs: 18000 \n",
      " mean_loss: 0.26464108\n",
      "eopch 3 - HR = 0.6389, NDCG = 0.3703 elapsed time: 57.75\n",
      "epochs: 4 batchs: 0 \n",
      " mean_loss: 0.29899573\n",
      "epochs: 4 batchs: 1000 \n",
      " mean_loss: 0.25089973\n",
      "epochs: 4 batchs: 2000 \n",
      " mean_loss: 0.25068688\n",
      "epochs: 4 batchs: 3000 \n",
      " mean_loss: 0.25127643\n",
      "epochs: 4 batchs: 4000 \n",
      " mean_loss: 0.25204203\n",
      "epochs: 4 batchs: 5000 \n",
      " mean_loss: 0.25251707\n",
      "epochs: 4 batchs: 6000 \n",
      " mean_loss: 0.2531107\n",
      "epochs: 4 batchs: 7000 \n",
      " mean_loss: 0.25341985\n",
      "epochs: 4 batchs: 8000 \n",
      " mean_loss: 0.25405067\n",
      "epochs: 4 batchs: 9000 \n",
      " mean_loss: 0.25437248\n",
      "epochs: 4 batchs: 10000 \n",
      " mean_loss: 0.25474977\n",
      "epochs: 4 batchs: 11000 \n",
      " mean_loss: 0.2551371\n",
      "epochs: 4 batchs: 12000 \n",
      " mean_loss: 0.2553265\n",
      "epochs: 4 batchs: 13000 \n",
      " mean_loss: 0.2556559\n",
      "epochs: 4 batchs: 14000 \n",
      " mean_loss: 0.2558189\n",
      "epochs: 4 batchs: 15000 \n",
      " mean_loss: 0.25608873\n",
      "epochs: 4 batchs: 16000 \n",
      " mean_loss: 0.25625005\n",
      "epochs: 4 batchs: 17000 \n",
      " mean_loss: 0.25633433\n",
      "epochs: 4 batchs: 18000 \n",
      " mean_loss: 0.2565046\n",
      "eopch 4 - HR = 0.6608, NDCG = 0.3831 elapsed time: 57.00\n",
      "epochs: 5 batchs: 0 \n",
      " mean_loss: 0.25697923\n",
      "epochs: 5 batchs: 1000 \n",
      " mean_loss: 0.24343954\n",
      "epochs: 5 batchs: 2000 \n",
      " mean_loss: 0.24490397\n",
      "epochs: 5 batchs: 3000 \n",
      " mean_loss: 0.24550596\n",
      "epochs: 5 batchs: 4000 \n",
      " mean_loss: 0.24623547\n",
      "epochs: 5 batchs: 5000 \n",
      " mean_loss: 0.24654639\n",
      "epochs: 5 batchs: 6000 \n",
      " mean_loss: 0.24699232\n",
      "epochs: 5 batchs: 7000 \n",
      " mean_loss: 0.24731937\n",
      "epochs: 5 batchs: 8000 \n",
      " mean_loss: 0.24774513\n",
      "epochs: 5 batchs: 9000 \n",
      " mean_loss: 0.24812359\n",
      "epochs: 5 batchs: 10000 \n",
      " mean_loss: 0.24854659\n",
      "epochs: 5 batchs: 11000 \n",
      " mean_loss: 0.24880737\n",
      "epochs: 5 batchs: 12000 \n",
      " mean_loss: 0.24896432\n",
      "epochs: 5 batchs: 13000 \n",
      " mean_loss: 0.24911478\n",
      "epochs: 5 batchs: 14000 \n",
      " mean_loss: 0.24941093\n",
      "epochs: 5 batchs: 15000 \n",
      " mean_loss: 0.24956682\n",
      "epochs: 5 batchs: 16000 \n",
      " mean_loss: 0.24977358\n",
      "epochs: 5 batchs: 17000 \n",
      " mean_loss: 0.24999467\n",
      "epochs: 5 batchs: 18000 \n",
      " mean_loss: 0.25006163\n",
      "eopch 5 - HR = 0.6611, NDCG = 0.3864 elapsed time: 58.08\n",
      "epochs: 6 batchs: 0 \n",
      " mean_loss: 0.2147794\n",
      "epochs: 6 batchs: 1000 \n",
      " mean_loss: 0.23827887\n",
      "epochs: 6 batchs: 2000 \n",
      " mean_loss: 0.23825853\n",
      "epochs: 6 batchs: 3000 \n",
      " mean_loss: 0.23876294\n",
      "epochs: 6 batchs: 4000 \n",
      " mean_loss: 0.23968764\n",
      "epochs: 6 batchs: 5000 \n",
      " mean_loss: 0.24032572\n",
      "epochs: 6 batchs: 6000 \n",
      " mean_loss: 0.24057823\n",
      "epochs: 6 batchs: 7000 \n",
      " mean_loss: 0.24090199\n",
      "epochs: 6 batchs: 8000 \n",
      " mean_loss: 0.24141157\n",
      "epochs: 6 batchs: 9000 \n",
      " mean_loss: 0.24171387\n",
      "epochs: 6 batchs: 10000 \n",
      " mean_loss: 0.24220124\n",
      "epochs: 6 batchs: 11000 \n",
      " mean_loss: 0.24274218\n",
      "epochs: 6 batchs: 12000 \n",
      " mean_loss: 0.24300937\n",
      "epochs: 6 batchs: 13000 \n",
      " mean_loss: 0.24311303\n",
      "epochs: 6 batchs: 14000 \n",
      " mean_loss: 0.24329619\n",
      "epochs: 6 batchs: 15000 \n",
      " mean_loss: 0.24360362\n",
      "epochs: 6 batchs: 16000 \n",
      " mean_loss: 0.24393012\n",
      "epochs: 6 batchs: 17000 \n",
      " mean_loss: 0.24422288\n",
      "epochs: 6 batchs: 18000 \n",
      " mean_loss: 0.24454142\n",
      "eopch 6 - HR = 0.6614, NDCG = 0.3880 elapsed time: 59.84\n",
      "epochs: 7 batchs: 0 \n",
      " mean_loss: 0.2294738\n",
      "epochs: 7 batchs: 1000 \n",
      " mean_loss: 0.23032804\n",
      "epochs: 7 batchs: 2000 \n",
      " mean_loss: 0.23154996\n",
      "epochs: 7 batchs: 3000 \n",
      " mean_loss: 0.23253423\n",
      "epochs: 7 batchs: 4000 \n",
      " mean_loss: 0.23390585\n",
      "epochs: 7 batchs: 5000 \n",
      " mean_loss: 0.2347701\n",
      "epochs: 7 batchs: 6000 \n",
      " mean_loss: 0.23536058\n",
      "epochs: 7 batchs: 7000 \n",
      " mean_loss: 0.23577254\n",
      "epochs: 7 batchs: 8000 \n",
      " mean_loss: 0.23627242\n",
      "epochs: 7 batchs: 9000 \n",
      " mean_loss: 0.23661658\n",
      "epochs: 7 batchs: 10000 \n",
      " mean_loss: 0.23713315\n",
      "epochs: 7 batchs: 11000 \n",
      " mean_loss: 0.2375528\n",
      "epochs: 7 batchs: 12000 \n",
      " mean_loss: 0.2380069\n",
      "epochs: 7 batchs: 13000 \n",
      " mean_loss: 0.23837282\n",
      "epochs: 7 batchs: 14000 \n",
      " mean_loss: 0.23870772\n",
      "epochs: 7 batchs: 15000 \n",
      " mean_loss: 0.23902385\n",
      "epochs: 7 batchs: 16000 \n",
      " mean_loss: 0.23926623\n",
      "epochs: 7 batchs: 17000 \n",
      " mean_loss: 0.2395207\n",
      "epochs: 7 batchs: 18000 \n",
      " mean_loss: 0.23970844\n",
      "eopch 7 - HR = 0.6584, NDCG = 0.3860 elapsed time: 62.16\n",
      "epochs: 8 batchs: 0 \n",
      " mean_loss: 0.26653937\n",
      "epochs: 8 batchs: 1000 \n",
      " mean_loss: 0.22784552\n",
      "epochs: 8 batchs: 2000 \n",
      " mean_loss: 0.22785787\n",
      "epochs: 8 batchs: 3000 \n",
      " mean_loss: 0.22864732\n",
      "epochs: 8 batchs: 4000 \n",
      " mean_loss: 0.22920881\n",
      "epochs: 8 batchs: 5000 \n",
      " mean_loss: 0.22959732\n",
      "epochs: 8 batchs: 6000 \n",
      " mean_loss: 0.2300527\n",
      "epochs: 8 batchs: 7000 \n",
      " mean_loss: 0.23085615\n",
      "epochs: 8 batchs: 8000 \n",
      " mean_loss: 0.23171791\n",
      "epochs: 8 batchs: 9000 \n",
      " mean_loss: 0.23232003\n",
      "epochs: 8 batchs: 10000 \n",
      " mean_loss: 0.23255663\n",
      "epochs: 8 batchs: 11000 \n",
      " mean_loss: 0.2330031\n",
      "epochs: 8 batchs: 12000 \n",
      " mean_loss: 0.23357888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 8 batchs: 13000 \n",
      " mean_loss: 0.23387367\n",
      "epochs: 8 batchs: 14000 \n",
      " mean_loss: 0.23420435\n",
      "epochs: 8 batchs: 15000 \n",
      " mean_loss: 0.2344695\n",
      "epochs: 8 batchs: 16000 \n",
      " mean_loss: 0.23475337\n",
      "epochs: 8 batchs: 17000 \n",
      " mean_loss: 0.23514767\n",
      "epochs: 8 batchs: 18000 \n",
      " mean_loss: 0.23546375\n",
      "eopch 8 - HR = 0.6609, NDCG = 0.3850 elapsed time: 62.27\n",
      "epochs: 9 batchs: 0 \n",
      " mean_loss: 0.25942355\n",
      "epochs: 9 batchs: 1000 \n",
      " mean_loss: 0.22155261\n",
      "epochs: 9 batchs: 2000 \n",
      " mean_loss: 0.22332694\n",
      "epochs: 9 batchs: 3000 \n",
      " mean_loss: 0.2237628\n",
      "epochs: 9 batchs: 4000 \n",
      " mean_loss: 0.22461642\n",
      "epochs: 9 batchs: 5000 \n",
      " mean_loss: 0.22541937\n",
      "epochs: 9 batchs: 6000 \n",
      " mean_loss: 0.22631463\n",
      "epochs: 9 batchs: 7000 \n",
      " mean_loss: 0.22683692\n",
      "epochs: 9 batchs: 8000 \n",
      " mean_loss: 0.22734119\n",
      "epochs: 9 batchs: 9000 \n",
      " mean_loss: 0.227892\n",
      "epochs: 9 batchs: 10000 \n",
      " mean_loss: 0.22849838\n",
      "epochs: 9 batchs: 11000 \n",
      " mean_loss: 0.22882833\n",
      "epochs: 9 batchs: 12000 \n",
      " mean_loss: 0.22939633\n",
      "epochs: 9 batchs: 13000 \n",
      " mean_loss: 0.22980477\n",
      "epochs: 9 batchs: 14000 \n",
      " mean_loss: 0.23026279\n",
      "epochs: 9 batchs: 15000 \n",
      " mean_loss: 0.2306122\n",
      "epochs: 9 batchs: 16000 \n",
      " mean_loss: 0.23099399\n",
      "epochs: 9 batchs: 17000 \n",
      " mean_loss: 0.2312089\n",
      "epochs: 9 batchs: 18000 \n",
      " mean_loss: 0.2314748\n",
      "eopch 9 - HR = 0.6656, NDCG = 0.3903 elapsed time: 62.44\n",
      "epochs: 10 batchs: 0 \n",
      " mean_loss: 0.20359501\n",
      "epochs: 10 batchs: 1000 \n",
      " mean_loss: 0.218052\n",
      "epochs: 10 batchs: 2000 \n",
      " mean_loss: 0.21988392\n",
      "epochs: 10 batchs: 3000 \n",
      " mean_loss: 0.22038703\n",
      "epochs: 10 batchs: 4000 \n",
      " mean_loss: 0.22129865\n",
      "epochs: 10 batchs: 5000 \n",
      " mean_loss: 0.22199991\n",
      "epochs: 10 batchs: 6000 \n",
      " mean_loss: 0.22265905\n",
      "epochs: 10 batchs: 7000 \n",
      " mean_loss: 0.2232097\n",
      "epochs: 10 batchs: 8000 \n",
      " mean_loss: 0.2240388\n",
      "epochs: 10 batchs: 9000 \n",
      " mean_loss: 0.22456382\n",
      "epochs: 10 batchs: 10000 \n",
      " mean_loss: 0.22494046\n",
      "epochs: 10 batchs: 11000 \n",
      " mean_loss: 0.22533013\n",
      "epochs: 10 batchs: 12000 \n",
      " mean_loss: 0.22574821\n",
      "epochs: 10 batchs: 13000 \n",
      " mean_loss: 0.22630297\n",
      "epochs: 10 batchs: 14000 \n",
      " mean_loss: 0.22675164\n",
      "epochs: 10 batchs: 15000 \n",
      " mean_loss: 0.22702946\n",
      "epochs: 10 batchs: 16000 \n",
      " mean_loss: 0.22742842\n",
      "epochs: 10 batchs: 17000 \n",
      " mean_loss: 0.22775395\n",
      "epochs: 10 batchs: 18000 \n",
      " mean_loss: 0.22803965\n",
      "eopch 10 - HR = 0.6621, NDCG = 0.3865 elapsed time: 61.54\n",
      "epochs: 11 batchs: 0 \n",
      " mean_loss: 0.24879117\n",
      "epochs: 11 batchs: 1000 \n",
      " mean_loss: 0.21625508\n",
      "epochs: 11 batchs: 2000 \n",
      " mean_loss: 0.21667877\n",
      "epochs: 11 batchs: 3000 \n",
      " mean_loss: 0.21782644\n",
      "epochs: 11 batchs: 4000 \n",
      " mean_loss: 0.21845713\n",
      "epochs: 11 batchs: 5000 \n",
      " mean_loss: 0.21947417\n",
      "epochs: 11 batchs: 6000 \n",
      " mean_loss: 0.22000083\n",
      "epochs: 11 batchs: 7000 \n",
      " mean_loss: 0.22044374\n",
      "epochs: 11 batchs: 8000 \n",
      " mean_loss: 0.22087738\n",
      "epochs: 11 batchs: 9000 \n",
      " mean_loss: 0.22117351\n",
      "epochs: 11 batchs: 10000 \n",
      " mean_loss: 0.2216134\n",
      "epochs: 11 batchs: 11000 \n",
      " mean_loss: 0.22221586\n",
      "epochs: 11 batchs: 12000 \n",
      " mean_loss: 0.22257483\n",
      "epochs: 11 batchs: 13000 \n",
      " mean_loss: 0.22314179\n",
      "epochs: 11 batchs: 14000 \n",
      " mean_loss: 0.22349171\n",
      "epochs: 11 batchs: 15000 \n",
      " mean_loss: 0.2239177\n",
      "epochs: 11 batchs: 16000 \n",
      " mean_loss: 0.22433287\n",
      "epochs: 11 batchs: 17000 \n",
      " mean_loss: 0.22457446\n",
      "epochs: 11 batchs: 18000 \n",
      " mean_loss: 0.22477734\n",
      "eopch 11 - HR = 0.6661, NDCG = 0.3903 elapsed time: 61.01\n",
      "epochs: 12 batchs: 0 \n",
      " mean_loss: 0.23377913\n",
      "epochs: 12 batchs: 1000 \n",
      " mean_loss: 0.21297821\n",
      "epochs: 12 batchs: 2000 \n",
      " mean_loss: 0.21346278\n",
      "epochs: 12 batchs: 3000 \n",
      " mean_loss: 0.21462692\n",
      "epochs: 12 batchs: 4000 \n",
      " mean_loss: 0.2149585\n",
      "epochs: 12 batchs: 5000 \n",
      " mean_loss: 0.21561557\n",
      "epochs: 12 batchs: 6000 \n",
      " mean_loss: 0.21619436\n",
      "epochs: 12 batchs: 7000 \n",
      " mean_loss: 0.21667095\n",
      "epochs: 12 batchs: 8000 \n",
      " mean_loss: 0.21719314\n",
      "epochs: 12 batchs: 9000 \n",
      " mean_loss: 0.21772672\n",
      "epochs: 12 batchs: 10000 \n",
      " mean_loss: 0.2182994\n",
      "epochs: 12 batchs: 11000 \n",
      " mean_loss: 0.21887517\n",
      "epochs: 12 batchs: 12000 \n",
      " mean_loss: 0.21930034\n",
      "epochs: 12 batchs: 13000 \n",
      " mean_loss: 0.21986808\n",
      "epochs: 12 batchs: 14000 \n",
      " mean_loss: 0.22036582\n",
      "epochs: 12 batchs: 15000 \n",
      " mean_loss: 0.22067204\n",
      "epochs: 12 batchs: 16000 \n",
      " mean_loss: 0.22100566\n",
      "epochs: 12 batchs: 17000 \n",
      " mean_loss: 0.22135127\n",
      "epochs: 12 batchs: 18000 \n",
      " mean_loss: 0.22171527\n",
      "eopch 12 - HR = 0.6608, NDCG = 0.3844 elapsed time: 61.52\n",
      "epochs: 13 batchs: 0 \n",
      " mean_loss: 0.19993651\n",
      "epochs: 13 batchs: 1000 \n",
      " mean_loss: 0.20992984\n",
      "epochs: 13 batchs: 2000 \n",
      " mean_loss: 0.21112575\n",
      "epochs: 13 batchs: 3000 \n",
      " mean_loss: 0.21188502\n",
      "epochs: 13 batchs: 4000 \n",
      " mean_loss: 0.21249138\n",
      "epochs: 13 batchs: 5000 \n",
      " mean_loss: 0.21316423\n",
      "epochs: 13 batchs: 6000 \n",
      " mean_loss: 0.21373168\n",
      "epochs: 13 batchs: 7000 \n",
      " mean_loss: 0.21432717\n",
      "epochs: 13 batchs: 8000 \n",
      " mean_loss: 0.21491818\n",
      "epochs: 13 batchs: 9000 \n",
      " mean_loss: 0.21523778\n",
      "epochs: 13 batchs: 10000 \n",
      " mean_loss: 0.21562077\n",
      "epochs: 13 batchs: 11000 \n",
      " mean_loss: 0.21613243\n",
      "epochs: 13 batchs: 12000 \n",
      " mean_loss: 0.21655267\n",
      "epochs: 13 batchs: 13000 \n",
      " mean_loss: 0.2170563\n",
      "epochs: 13 batchs: 14000 \n",
      " mean_loss: 0.21748799\n",
      "epochs: 13 batchs: 15000 \n",
      " mean_loss: 0.21785143\n",
      "epochs: 13 batchs: 16000 \n",
      " mean_loss: 0.21811692\n",
      "epochs: 13 batchs: 17000 \n",
      " mean_loss: 0.21855544\n",
      "epochs: 13 batchs: 18000 \n",
      " mean_loss: 0.2188389\n",
      "eopch 13 - HR = 0.6530, NDCG = 0.3817 elapsed time: 62.04\n",
      "epochs: 14 batchs: 0 \n",
      " mean_loss: 0.20125207\n",
      "epochs: 14 batchs: 1000 \n",
      " mean_loss: 0.20606597\n",
      "epochs: 14 batchs: 2000 \n",
      " mean_loss: 0.20728573\n",
      "epochs: 14 batchs: 3000 \n",
      " mean_loss: 0.20841487\n",
      "epochs: 14 batchs: 4000 \n",
      " mean_loss: 0.20922688\n",
      "epochs: 14 batchs: 5000 \n",
      " mean_loss: 0.21006018\n",
      "epochs: 14 batchs: 6000 \n",
      " mean_loss: 0.21058807\n",
      "epochs: 14 batchs: 7000 \n",
      " mean_loss: 0.21134955\n",
      "epochs: 14 batchs: 8000 \n",
      " mean_loss: 0.21186925\n",
      "epochs: 14 batchs: 9000 \n",
      " mean_loss: 0.21253276\n",
      "epochs: 14 batchs: 10000 \n",
      " mean_loss: 0.21293466\n",
      "epochs: 14 batchs: 11000 \n",
      " mean_loss: 0.21336608\n",
      "epochs: 14 batchs: 12000 \n",
      " mean_loss: 0.21388243\n",
      "epochs: 14 batchs: 13000 \n",
      " mean_loss: 0.21426047\n",
      "epochs: 14 batchs: 14000 \n",
      " mean_loss: 0.21467735\n",
      "epochs: 14 batchs: 15000 \n",
      " mean_loss: 0.21505848\n",
      "epochs: 14 batchs: 16000 \n",
      " mean_loss: 0.21551056\n",
      "epochs: 14 batchs: 17000 \n",
      " mean_loss: 0.21593682\n",
      "epochs: 14 batchs: 18000 \n",
      " mean_loss: 0.2163212\n",
      "eopch 14 - HR = 0.6480, NDCG = 0.3813 elapsed time: 61.38\n",
      "epochs: 15 batchs: 0 \n",
      " mean_loss: 0.20964622\n",
      "epochs: 15 batchs: 1000 \n",
      " mean_loss: 0.20323311\n",
      "epochs: 15 batchs: 2000 \n",
      " mean_loss: 0.20345855\n",
      "epochs: 15 batchs: 3000 \n",
      " mean_loss: 0.20500308\n",
      "epochs: 15 batchs: 4000 \n",
      " mean_loss: 0.206271\n",
      "epochs: 15 batchs: 5000 \n",
      " mean_loss: 0.20710929\n",
      "epochs: 15 batchs: 6000 \n",
      " mean_loss: 0.20781225\n",
      "epochs: 15 batchs: 7000 \n",
      " mean_loss: 0.20876081\n",
      "epochs: 15 batchs: 8000 \n",
      " mean_loss: 0.2095079\n",
      "epochs: 15 batchs: 9000 \n",
      " mean_loss: 0.21015811\n",
      "epochs: 15 batchs: 10000 \n",
      " mean_loss: 0.2107828\n",
      "epochs: 15 batchs: 11000 \n",
      " mean_loss: 0.21126549\n",
      "epochs: 15 batchs: 12000 \n",
      " mean_loss: 0.21172825\n",
      "epochs: 15 batchs: 13000 \n",
      " mean_loss: 0.21210966\n",
      "epochs: 15 batchs: 14000 \n",
      " mean_loss: 0.21246716\n",
      "epochs: 15 batchs: 15000 \n",
      " mean_loss: 0.21284635\n",
      "epochs: 15 batchs: 16000 \n",
      " mean_loss: 0.21325453\n",
      "epochs: 15 batchs: 17000 \n",
      " mean_loss: 0.21361333\n",
      "epochs: 15 batchs: 18000 \n",
      " mean_loss: 0.2138996\n",
      "eopch 15 - HR = 0.6586, NDCG = 0.3843 elapsed time: 61.74\n",
      "epochs: 16 batchs: 0 \n",
      " mean_loss: 0.17611589\n",
      "epochs: 16 batchs: 1000 \n",
      " mean_loss: 0.20164151\n",
      "epochs: 16 batchs: 2000 \n",
      " mean_loss: 0.2016364\n",
      "epochs: 16 batchs: 3000 \n",
      " mean_loss: 0.20294735\n",
      "epochs: 16 batchs: 4000 \n",
      " mean_loss: 0.20410419\n",
      "epochs: 16 batchs: 5000 \n",
      " mean_loss: 0.20451869\n",
      "epochs: 16 batchs: 6000 \n",
      " mean_loss: 0.20525669\n",
      "epochs: 16 batchs: 7000 \n",
      " mean_loss: 0.20578383\n",
      "epochs: 16 batchs: 8000 \n",
      " mean_loss: 0.20644009\n",
      "epochs: 16 batchs: 9000 \n",
      " mean_loss: 0.2073303\n",
      "epochs: 16 batchs: 10000 \n",
      " mean_loss: 0.20793185\n",
      "epochs: 16 batchs: 11000 \n",
      " mean_loss: 0.20865035\n",
      "epochs: 16 batchs: 12000 \n",
      " mean_loss: 0.20914985\n",
      "epochs: 16 batchs: 13000 \n",
      " mean_loss: 0.20960009\n",
      "epochs: 16 batchs: 14000 \n",
      " mean_loss: 0.21013682\n",
      "epochs: 16 batchs: 15000 \n",
      " mean_loss: 0.2105112\n",
      "epochs: 16 batchs: 16000 \n",
      " mean_loss: 0.21089633\n",
      "epochs: 16 batchs: 17000 \n",
      " mean_loss: 0.2113283\n",
      "epochs: 16 batchs: 18000 \n",
      " mean_loss: 0.21169668\n",
      "eopch 16 - HR = 0.6545, NDCG = 0.3830 elapsed time: 61.82\n",
      "max patience, training stop\n",
      "max hit ratio: 0.6660596026490067 max ndcg ratio: 0.3902524389144955\n"
     ]
    }
   ],
   "source": [
    "## Train model\n",
    "### early stopping을 위한 값 세팅\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    ### data shuffle\n",
    "    user_input, item_input, labels = shuffle(\n",
    "        user_input, item_input, labels\n",
    "    )\n",
    "    ### training\n",
    "    batch_len = len(user_input) // batch_size\n",
    "    batch_loss = list()\n",
    "    for i in range(batch_len):\n",
    "        start = i*batch_size\n",
    "        user_input_bc = user_input[start : start+batch_size]\n",
    "        item_input_bc = item_input[start : start+batch_size]\n",
    "        labels_bc = labels[start : start+batch_size]\n",
    "    \n",
    "        _, l = sess.run([train_op, loss], feed_dict={\n",
    "            user_input_ph: user_input_bc,\n",
    "            item_input_ph: item_input_bc,\n",
    "            labels_ph: labels_bc\n",
    "        })\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        if i % 1000==0:\n",
    "            print(\"epochs:\", epoch, \"batchs:\", i, \"\\n\", \"mean_loss:\", \n",
    "                  np.array(batch_loss).mean())\n",
    "    \n",
    "    ### evaluation\n",
    "    hits, ndcgs = evaluation(sess, testRatings, testNegatives, topK)\n",
    "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print('eopch', epoch, '- HR = %.4f, NDCG = %.4f' % (hr, ndcg),\n",
    "         'elapsed time: %.2f' % (time()-t1))\n",
    "    ### HitRatio가 이전 값보다 작아지면 종료한다.\n",
    "    if hr > best_hr:\n",
    "        best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience == max_patience:\n",
    "            print(\"max patience, training stop\")\n",
    "            print(\"max hit ratio:\", best_hr, \"max ndcg ratio:\", best_ndcg)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
