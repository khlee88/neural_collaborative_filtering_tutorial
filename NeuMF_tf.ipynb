{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "dataset = 'ml-1m'\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "num_factors = 32\n",
    "reg_mf = 0.0\n",
    "layers = [128,64,64,32]\n",
    "reg_layers = [0.,0.,0.,0.]\n",
    "num_neg = 4\n",
    "lr = 0.001\n",
    "patience = 0\n",
    "max_patience = 5\n",
    "topK = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load train data \n",
    "### user, item, rating, time 으로 구성된 raw data로 부터\n",
    "### (user, item) value 1인 implicit feedback dictionary로 변환\n",
    "trn = pd.read_csv(path+dataset+\".train.rating\", sep=\"\\t\", names=['user','item','rating','time'])\n",
    "num_users=max(trn.user) + 1\n",
    "num_items=max(trn.item) + 1\n",
    "trn = trn[trn['rating']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load test data\n",
    "### testRatings는 [user, item] list\n",
    "tst = pd.read_csv(\"data/ml-1m.test.rating\", sep=\"\\t\", names=['user','item'], usecols = [0,1])\n",
    "testRatings = tst.values.tolist()\n",
    "### testNegatives는 [item_list]이고 index는 user와 동일\n",
    "neg = pd.read_csv(\"data/ml-1m.test.negative\", sep=\"_\", names=['neg_list'])\n",
    "testNegatives = neg.neg_list.map(lambda x: list(map(int, x.split('\\t')[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  item  rating       time\n",
      "0     0    32       4  978824330\n",
      "1     0    34       4  978824330\n",
      "2     0     4       5  978824291\n",
      "[[0, 25], [1, 133], [2, 207], [3, 208], [4, 222]]\n",
      "[1064, 174, 2791, 3373, 269, 2678, 1902, 3641, 1216, 915, 3672, 2803, 2344, 986, 3217, 2824, 2598, 464, 2340, 1952, 1855, 1353, 1547, 3487, 3293, 1541, 2414, 2728, 340, 1421, 1963, 2545, 972, 487, 3463, 2727, 1135, 3135, 128, 175, 2423, 1974, 2515, 3278, 3079, 1527, 2182, 1018, 2800, 1830, 1539, 617, 247, 3448, 1699, 1420, 2487, 198, 811, 1010, 1423, 2840, 1770, 881, 1913, 1803, 1734, 3326, 1617, 224, 3352, 1869, 1182, 1331, 336, 2517, 1721, 3512, 3656, 273, 1026, 1991, 2190, 998, 3386, 3369, 185, 2822, 864, 2854, 3067, 58, 2551, 2333, 2688, 3703, 1300, 1924, 3118]\n"
     ]
    }
   ],
   "source": [
    "print(trn[:3])\n",
    "print(testRatings[0:5])\n",
    "print(testNegatives[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train Negative sampling and Make Input Data\n",
    "### user 별 item list 데이터를 만든다.\n",
    "trn_ = trn.groupby('user').item.agg(lambda x: list(x))\n",
    "### user 별 item list에서 test set의 item이 아닌 item 중에서 negative item sampling을 수행한다.\n",
    "item_input = []\n",
    "labels = []\n",
    "for u, (l, n, t) in enumerate(zip(trn_, testNegatives, testRatings)):\n",
    "    ## l은 train item, n은 test neg item, t[1]은 tst item으로\n",
    "    ## train & test set에 포함되지 않는 item 중 num_neg 배수만큼 추출한다.\n",
    "    ## 즉, 1번 user가 100개의 item이 있다면 neg item은 400개를 추출 하는 것이다.\n",
    "    neg_set = list(set(range(num_items)) - set(l + n + [t[1]]))\n",
    "    neg_set = random.sample(neg_set, min(len(l)*num_neg, len(neg_set)))\n",
    "    ## train positive, negative item list를 구성하고 그에 맞게 label list를 구성한다.\n",
    "    item_input.append(l + neg_set)\n",
    "    labels.append([1]*len(l) + [0]*len(neg_set))\n",
    "### item_input에 맞에 user_input을 구성한다. \n",
    "user_input = [[i]*len(l) for i, l in enumerate(item_input)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2d list to 1d list\n",
    "user_input = [j for sub in user_input for j in sub]\n",
    "item_input = [j for sub in item_input for j in sub]\n",
    "labels = [j for sub in labels for j in sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## placeholder\n",
    "user_input_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='user_input')\n",
    "item_input_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='itme_input')\n",
    "labels_ph = tf.placeholder(shape=[None], dtype=tf.int32, name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "## user and item embedding matrix\n",
    "### mlp embedding의  첫번째 layer 노드 수의 1/2씩을 user, item의 embedding size로 생성\n",
    "with tf.variable_scope(\"gmf_embedding\", reuse=tf.AUTO_REUSE):\n",
    "    MF_Embedding_User = tf.get_variable(name=\"mf_embedding_users\", shape=[num_users, num_factors],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    regularizer=tf.contrib.layers.l2_regularizer(reg_mf))\n",
    "    MF_Embedding_Item = tf.get_variable(name=\"mf_embedding_items\", shape=[num_items, num_factors],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    regularizer=tf.contrib.layers.l2_regularizer(reg_mf))\n",
    "    \n",
    "with tf.variable_scope(\"mlp_embedding\", reuse=tf.AUTO_REUSE):\n",
    "    MLP_Embedding_User = tf.get_variable(name=\"mlp_embedding_users\", shape=[num_users, layers[0]/2],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    regularizer=tf.contrib.layers.l2_regularizer(reg_layers[0]))\n",
    "    MLP_Embedding_Item = tf.get_variable(name=\"mlp_embedding_items\", shape=[num_items, layers[0]/2],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    regularizer=tf.contrib.layers.l2_regularizer(reg_layers[0]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "## prediction for user, item set\n",
    "### MF part\n",
    "mf_user_latent = tf.nn.embedding_lookup(MF_Embedding_User, user_input_ph)\n",
    "mf_item_latent = tf.nn.embedding_lookup(MF_Embedding_Item, item_input_ph)\n",
    "mf_vector = tf.multiply(mf_user_latent, mf_item_latent)\n",
    "\n",
    "### MLP part \n",
    "mlp_user_latent = tf.nn.embedding_lookup(MLP_Embedding_User, user_input_ph)\n",
    "mlp_item_latent = tf.nn.embedding_lookup(MLP_Embedding_Item, item_input_ph)\n",
    "mlp_vector = tf.concat([mlp_user_latent, mlp_item_latent], 1) \n",
    "num_layer = len(layers)\n",
    "for idx in range(1, num_layer):\n",
    "    mlp_vector = tf.contrib.layers.fully_connected(\n",
    "        mlp_vector, layers[idx], activation_fn=tf.nn.relu,\n",
    "        weights_regularizer=tf.contrib.layers.l2_regularizer(reg_layers[idx])\n",
    "    )\n",
    "    \n",
    "### concat MF, MLP\n",
    "predict_vector = tf.concat([mf_vector, mlp_vector], 1) \n",
    "\n",
    "### final prediction layer\n",
    "prediction = tf.contrib.layers.fully_connected(predict_vector, 1, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loss and optimizer\n",
    "### label shape을 prediction과 동일하게 [None, 1]로 변경해주고, loss(cost)를 계산한다.\n",
    "loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(tf.reshape(labels_ph, [-1,1]), tf.float32), \n",
    "            logits=prediction)\n",
    ")\n",
    "## Adam Optimizer \n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### test set의 구성은 user별 positive item 1개와 negetive item 99개로 구성되어 있다.\n",
    "### evaluaton process\n",
    "### 1. 유저별 neg, pos 100개의 item에 대하여 model에 input하여 prediction을 계산한다.\n",
    "### 2. 100개의 prediction 값 중 내림차순으로 상위 topK(10개) item을 순서를 유지하여 추출한다.\n",
    "### 3. HitRatio는 10개 중 positive item이 있으면 1 없으면 0으로 값을 return한다.\n",
    "### 4. NDCG는 positive item의 순위를 고려하여 계산한것이다. 즉, rank 1이라면 값은 1이 되고,\n",
    "###    rank가 2, 3... 뒤로 갈 수록 log비율로 감소시킨다. 10개 중 없으면 0을 리턴함.\n",
    "### 5. user별 HitRatio와 NDCG 값을 list로 구성하고 평균을 구하면 최종 평가 지표가 된다. \n",
    "def evaluation(sess, testRatings, testNegatives, K):\n",
    "    def getHitRatio(ranklist, gtItem):\n",
    "        for item in ranklist:\n",
    "            if item == gtItem:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def getNDCG(ranklist, gtItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == gtItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "    \n",
    "    hits, ndcgs = [],[]\n",
    "    for i in range(len(testRatings)):\n",
    "        rating = testRatings[i]\n",
    "        items = testNegatives[i]\n",
    "        u = rating[0]\n",
    "        gtItem = rating[1]\n",
    "        items.append(gtItem)\n",
    "        \n",
    "        # Get prediction scores\n",
    "        map_item_score = {}\n",
    "        users = np.full(len(items), u, dtype = 'int32')\n",
    "        predictions = sess.run(prediction, \n",
    "                               feed_dict={user_input_ph: users, item_input_ph: items})\n",
    "\n",
    "        for i in range(len(items)):\n",
    "            item = items[i]\n",
    "            map_item_score[item] = predictions[i]\n",
    "        items.pop()\n",
    "        \n",
    "        # Evaluate top rank list\n",
    "        ranklist = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        \n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    \n",
    "    return(hits, ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## session start\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.0945, NDCG = 0.0425\n"
     ]
    }
   ],
   "source": [
    "## initial HitRatio and NDCG evaluation\n",
    "hits, ndcgs = evaluation(sess, testRatings, testNegatives, topK)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f' % (hr, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 batchs: 0 \n",
      " mean_loss: 0.6923248\n",
      "epochs: 0 batchs: 1000 \n",
      " mean_loss: 0.39575312\n",
      "epochs: 0 batchs: 2000 \n",
      " mean_loss: 0.3831697\n",
      "epochs: 0 batchs: 3000 \n",
      " mean_loss: 0.37642172\n",
      "epochs: 0 batchs: 4000 \n",
      " mean_loss: 0.36915818\n",
      "epochs: 0 batchs: 5000 \n",
      " mean_loss: 0.36150393\n",
      "epochs: 0 batchs: 6000 \n",
      " mean_loss: 0.3548257\n",
      "epochs: 0 batchs: 7000 \n",
      " mean_loss: 0.34877303\n",
      "epochs: 0 batchs: 8000 \n",
      " mean_loss: 0.3437354\n",
      "epochs: 0 batchs: 9000 \n",
      " mean_loss: 0.33887035\n",
      "epochs: 0 batchs: 10000 \n",
      " mean_loss: 0.3348629\n",
      "epochs: 0 batchs: 11000 \n",
      " mean_loss: 0.3311982\n",
      "epochs: 0 batchs: 12000 \n",
      " mean_loss: 0.32791173\n",
      "epochs: 0 batchs: 13000 \n",
      " mean_loss: 0.32481402\n",
      "epochs: 0 batchs: 14000 \n",
      " mean_loss: 0.32216352\n",
      "epochs: 0 batchs: 15000 \n",
      " mean_loss: 0.31972203\n",
      "epochs: 0 batchs: 16000 \n",
      " mean_loss: 0.31740117\n",
      "epochs: 0 batchs: 17000 \n",
      " mean_loss: 0.31521845\n",
      "epochs: 0 batchs: 18000 \n",
      " mean_loss: 0.31320068\n",
      "eopch 0 - HR = 0.6273, NDCG = 0.3611 elapsed time: 73.97\n",
      "epochs: 1 batchs: 0 \n",
      " mean_loss: 0.240859\n",
      "epochs: 1 batchs: 1000 \n",
      " mean_loss: 0.25568742\n",
      "epochs: 1 batchs: 2000 \n",
      " mean_loss: 0.25690502\n",
      "epochs: 1 batchs: 3000 \n",
      " mean_loss: 0.25735587\n",
      "epochs: 1 batchs: 4000 \n",
      " mean_loss: 0.2581753\n",
      "epochs: 1 batchs: 5000 \n",
      " mean_loss: 0.2581962\n",
      "epochs: 1 batchs: 6000 \n",
      " mean_loss: 0.25814644\n",
      "epochs: 1 batchs: 7000 \n",
      " mean_loss: 0.25809783\n",
      "epochs: 1 batchs: 8000 \n",
      " mean_loss: 0.25828192\n",
      "epochs: 1 batchs: 9000 \n",
      " mean_loss: 0.25827667\n",
      "epochs: 1 batchs: 10000 \n",
      " mean_loss: 0.25833306\n",
      "epochs: 1 batchs: 11000 \n",
      " mean_loss: 0.25831285\n",
      "epochs: 1 batchs: 12000 \n",
      " mean_loss: 0.25829867\n",
      "epochs: 1 batchs: 13000 \n",
      " mean_loss: 0.2582738\n",
      "epochs: 1 batchs: 14000 \n",
      " mean_loss: 0.2582868\n",
      "epochs: 1 batchs: 15000 \n",
      " mean_loss: 0.2582295\n",
      "epochs: 1 batchs: 16000 \n",
      " mean_loss: 0.25826207\n",
      "epochs: 1 batchs: 17000 \n",
      " mean_loss: 0.25817236\n",
      "epochs: 1 batchs: 18000 \n",
      " mean_loss: 0.25807005\n",
      "eopch 1 - HR = 0.6666, NDCG = 0.3899 elapsed time: 73.00\n",
      "epochs: 2 batchs: 0 \n",
      " mean_loss: 0.21391176\n",
      "epochs: 2 batchs: 1000 \n",
      " mean_loss: 0.2323065\n",
      "epochs: 2 batchs: 2000 \n",
      " mean_loss: 0.23197621\n",
      "epochs: 2 batchs: 3000 \n",
      " mean_loss: 0.23326665\n",
      "epochs: 2 batchs: 4000 \n",
      " mean_loss: 0.2341246\n",
      "epochs: 2 batchs: 5000 \n",
      " mean_loss: 0.23492181\n",
      "epochs: 2 batchs: 6000 \n",
      " mean_loss: 0.23541178\n",
      "epochs: 2 batchs: 7000 \n",
      " mean_loss: 0.23623177\n",
      "epochs: 2 batchs: 8000 \n",
      " mean_loss: 0.23678237\n",
      "epochs: 2 batchs: 9000 \n",
      " mean_loss: 0.23751801\n",
      "epochs: 2 batchs: 10000 \n",
      " mean_loss: 0.23788571\n",
      "epochs: 2 batchs: 11000 \n",
      " mean_loss: 0.2385215\n",
      "epochs: 2 batchs: 12000 \n",
      " mean_loss: 0.23896076\n",
      "epochs: 2 batchs: 13000 \n",
      " mean_loss: 0.2393258\n",
      "epochs: 2 batchs: 14000 \n",
      " mean_loss: 0.23970267\n",
      "epochs: 2 batchs: 15000 \n",
      " mean_loss: 0.23992296\n",
      "epochs: 2 batchs: 16000 \n",
      " mean_loss: 0.2401898\n",
      "epochs: 2 batchs: 17000 \n",
      " mean_loss: 0.24054188\n",
      "epochs: 2 batchs: 18000 \n",
      " mean_loss: 0.24080381\n",
      "eopch 2 - HR = 0.6682, NDCG = 0.3957 elapsed time: 73.85\n",
      "epochs: 3 batchs: 0 \n",
      " mean_loss: 0.23021056\n",
      "epochs: 3 batchs: 1000 \n",
      " mean_loss: 0.21713616\n",
      "epochs: 3 batchs: 2000 \n",
      " mean_loss: 0.21959147\n",
      "epochs: 3 batchs: 3000 \n",
      " mean_loss: 0.22006246\n",
      "epochs: 3 batchs: 4000 \n",
      " mean_loss: 0.22129972\n",
      "epochs: 3 batchs: 5000 \n",
      " mean_loss: 0.2224273\n",
      "epochs: 3 batchs: 6000 \n",
      " mean_loss: 0.22326349\n",
      "epochs: 3 batchs: 7000 \n",
      " mean_loss: 0.22418477\n",
      "epochs: 3 batchs: 8000 \n",
      " mean_loss: 0.22504205\n",
      "epochs: 3 batchs: 9000 \n",
      " mean_loss: 0.22584751\n",
      "epochs: 3 batchs: 10000 \n",
      " mean_loss: 0.22673602\n",
      "epochs: 3 batchs: 11000 \n",
      " mean_loss: 0.22748001\n",
      "epochs: 3 batchs: 12000 \n",
      " mean_loss: 0.2280918\n",
      "epochs: 3 batchs: 13000 \n",
      " mean_loss: 0.22869264\n",
      "epochs: 3 batchs: 14000 \n",
      " mean_loss: 0.22930983\n",
      "epochs: 3 batchs: 15000 \n",
      " mean_loss: 0.22972515\n",
      "epochs: 3 batchs: 16000 \n",
      " mean_loss: 0.23016758\n",
      "epochs: 3 batchs: 17000 \n",
      " mean_loss: 0.23059909\n",
      "epochs: 3 batchs: 18000 \n",
      " mean_loss: 0.23092616\n",
      "eopch 3 - HR = 0.6695, NDCG = 0.3971 elapsed time: 72.75\n",
      "epochs: 4 batchs: 0 \n",
      " mean_loss: 0.17946574\n",
      "epochs: 4 batchs: 1000 \n",
      " mean_loss: 0.20823653\n",
      "epochs: 4 batchs: 2000 \n",
      " mean_loss: 0.21016717\n",
      "epochs: 4 batchs: 3000 \n",
      " mean_loss: 0.21217859\n",
      "epochs: 4 batchs: 4000 \n",
      " mean_loss: 0.21332882\n",
      "epochs: 4 batchs: 5000 \n",
      " mean_loss: 0.2145203\n",
      "epochs: 4 batchs: 6000 \n",
      " mean_loss: 0.21586478\n",
      "epochs: 4 batchs: 7000 \n",
      " mean_loss: 0.2167006\n",
      "epochs: 4 batchs: 8000 \n",
      " mean_loss: 0.21736683\n",
      "epochs: 4 batchs: 9000 \n",
      " mean_loss: 0.21821931\n",
      "epochs: 4 batchs: 10000 \n",
      " mean_loss: 0.21921624\n",
      "epochs: 4 batchs: 11000 \n",
      " mean_loss: 0.2198563\n",
      "epochs: 4 batchs: 12000 \n",
      " mean_loss: 0.22057769\n",
      "epochs: 4 batchs: 13000 \n",
      " mean_loss: 0.22124743\n",
      "epochs: 4 batchs: 14000 \n",
      " mean_loss: 0.22180957\n",
      "epochs: 4 batchs: 15000 \n",
      " mean_loss: 0.2224304\n",
      "epochs: 4 batchs: 16000 \n",
      " mean_loss: 0.2228405\n",
      "epochs: 4 batchs: 17000 \n",
      " mean_loss: 0.22332728\n",
      "epochs: 4 batchs: 18000 \n",
      " mean_loss: 0.22390726\n",
      "eopch 4 - HR = 0.6740, NDCG = 0.4006 elapsed time: 73.83\n",
      "epochs: 5 batchs: 0 \n",
      " mean_loss: 0.17327172\n",
      "epochs: 5 batchs: 1000 \n",
      " mean_loss: 0.20351376\n",
      "epochs: 5 batchs: 2000 \n",
      " mean_loss: 0.20454635\n",
      "epochs: 5 batchs: 3000 \n",
      " mean_loss: 0.20545304\n",
      "epochs: 5 batchs: 4000 \n",
      " mean_loss: 0.20648012\n",
      "epochs: 5 batchs: 5000 \n",
      " mean_loss: 0.20772311\n",
      "epochs: 5 batchs: 6000 \n",
      " mean_loss: 0.20887563\n",
      "epochs: 5 batchs: 7000 \n",
      " mean_loss: 0.2101329\n",
      "epochs: 5 batchs: 8000 \n",
      " mean_loss: 0.21108519\n",
      "epochs: 5 batchs: 9000 \n",
      " mean_loss: 0.21213232\n",
      "epochs: 5 batchs: 10000 \n",
      " mean_loss: 0.21298265\n",
      "epochs: 5 batchs: 11000 \n",
      " mean_loss: 0.2136026\n",
      "epochs: 5 batchs: 12000 \n",
      " mean_loss: 0.21430878\n",
      "epochs: 5 batchs: 13000 \n",
      " mean_loss: 0.21524853\n",
      "epochs: 5 batchs: 14000 \n",
      " mean_loss: 0.21587132\n",
      "epochs: 5 batchs: 15000 \n",
      " mean_loss: 0.21647836\n",
      "epochs: 5 batchs: 16000 \n",
      " mean_loss: 0.21711415\n",
      "epochs: 5 batchs: 17000 \n",
      " mean_loss: 0.21766105\n",
      "epochs: 5 batchs: 18000 \n",
      " mean_loss: 0.21822013\n",
      "eopch 5 - HR = 0.6609, NDCG = 0.3941 elapsed time: 73.33\n",
      "epochs: 6 batchs: 0 \n",
      " mean_loss: 0.20687643\n",
      "epochs: 6 batchs: 1000 \n",
      " mean_loss: 0.19840239\n",
      "epochs: 6 batchs: 2000 \n",
      " mean_loss: 0.1998985\n",
      "epochs: 6 batchs: 3000 \n",
      " mean_loss: 0.200321\n",
      "epochs: 6 batchs: 4000 \n",
      " mean_loss: 0.20144124\n",
      "epochs: 6 batchs: 5000 \n",
      " mean_loss: 0.20297188\n",
      "epochs: 6 batchs: 6000 \n",
      " mean_loss: 0.20404306\n",
      "epochs: 6 batchs: 7000 \n",
      " mean_loss: 0.20535299\n",
      "epochs: 6 batchs: 8000 \n",
      " mean_loss: 0.2064166\n",
      "epochs: 6 batchs: 9000 \n",
      " mean_loss: 0.20739834\n",
      "epochs: 6 batchs: 10000 \n",
      " mean_loss: 0.20812887\n",
      "epochs: 6 batchs: 11000 \n",
      " mean_loss: 0.20898324\n",
      "epochs: 6 batchs: 12000 \n",
      " mean_loss: 0.2096909\n",
      "epochs: 6 batchs: 13000 \n",
      " mean_loss: 0.21033618\n",
      "epochs: 6 batchs: 14000 \n",
      " mean_loss: 0.21092717\n",
      "epochs: 6 batchs: 15000 \n",
      " mean_loss: 0.21160732\n",
      "epochs: 6 batchs: 16000 \n",
      " mean_loss: 0.2122857\n",
      "epochs: 6 batchs: 17000 \n",
      " mean_loss: 0.21277912\n",
      "epochs: 6 batchs: 18000 \n",
      " mean_loss: 0.21331248\n",
      "eopch 6 - HR = 0.6598, NDCG = 0.3910 elapsed time: 74.56\n",
      "epochs: 7 batchs: 0 \n",
      " mean_loss: 0.20934543\n",
      "epochs: 7 batchs: 1000 \n",
      " mean_loss: 0.1927283\n",
      "epochs: 7 batchs: 2000 \n",
      " mean_loss: 0.19404642\n",
      "epochs: 7 batchs: 3000 \n",
      " mean_loss: 0.19563618\n",
      "epochs: 7 batchs: 4000 \n",
      " mean_loss: 0.19723783\n",
      "epochs: 7 batchs: 5000 \n",
      " mean_loss: 0.19853424\n",
      "epochs: 7 batchs: 6000 \n",
      " mean_loss: 0.19964391\n",
      "epochs: 7 batchs: 7000 \n",
      " mean_loss: 0.20076182\n",
      "epochs: 7 batchs: 8000 \n",
      " mean_loss: 0.20180146\n",
      "epochs: 7 batchs: 9000 \n",
      " mean_loss: 0.20258576\n",
      "epochs: 7 batchs: 10000 \n",
      " mean_loss: 0.20344576\n",
      "epochs: 7 batchs: 11000 \n",
      " mean_loss: 0.2042207\n",
      "epochs: 7 batchs: 12000 \n",
      " mean_loss: 0.20497464\n",
      "epochs: 7 batchs: 13000 \n",
      " mean_loss: 0.2057438\n",
      "epochs: 7 batchs: 14000 \n",
      " mean_loss: 0.20646185\n",
      "epochs: 7 batchs: 15000 \n",
      " mean_loss: 0.20702484\n",
      "epochs: 7 batchs: 16000 \n",
      " mean_loss: 0.20770714\n",
      "epochs: 7 batchs: 17000 \n",
      " mean_loss: 0.20822327\n",
      "epochs: 7 batchs: 18000 \n",
      " mean_loss: 0.20875281\n",
      "eopch 7 - HR = 0.6560, NDCG = 0.3843 elapsed time: 74.00\n",
      "epochs: 8 batchs: 0 \n",
      " mean_loss: 0.16526479\n",
      "epochs: 8 batchs: 1000 \n",
      " mean_loss: 0.18818949\n",
      "epochs: 8 batchs: 2000 \n",
      " mean_loss: 0.18985768\n",
      "epochs: 8 batchs: 3000 \n",
      " mean_loss: 0.19098307\n",
      "epochs: 8 batchs: 4000 \n",
      " mean_loss: 0.19263779\n",
      "epochs: 8 batchs: 5000 \n",
      " mean_loss: 0.1937748\n",
      "epochs: 8 batchs: 6000 \n",
      " mean_loss: 0.19518317\n",
      "epochs: 8 batchs: 7000 \n",
      " mean_loss: 0.19633034\n",
      "epochs: 8 batchs: 8000 \n",
      " mean_loss: 0.1975091\n",
      "epochs: 8 batchs: 9000 \n",
      " mean_loss: 0.19838275\n",
      "epochs: 8 batchs: 10000 \n",
      " mean_loss: 0.19928837\n",
      "epochs: 8 batchs: 11000 \n",
      " mean_loss: 0.19991969\n",
      "epochs: 8 batchs: 12000 \n",
      " mean_loss: 0.20068309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 8 batchs: 13000 \n",
      " mean_loss: 0.20143138\n",
      "epochs: 8 batchs: 14000 \n",
      " mean_loss: 0.2021008\n",
      "epochs: 8 batchs: 15000 \n",
      " mean_loss: 0.20275941\n",
      "epochs: 8 batchs: 16000 \n",
      " mean_loss: 0.20336038\n",
      "epochs: 8 batchs: 17000 \n",
      " mean_loss: 0.20397489\n",
      "epochs: 8 batchs: 18000 \n",
      " mean_loss: 0.20463163\n",
      "eopch 8 - HR = 0.6487, NDCG = 0.3799 elapsed time: 74.13\n",
      "epochs: 9 batchs: 0 \n",
      " mean_loss: 0.15843788\n",
      "epochs: 9 batchs: 1000 \n",
      " mean_loss: 0.18282816\n",
      "epochs: 9 batchs: 2000 \n",
      " mean_loss: 0.18497096\n",
      "epochs: 9 batchs: 3000 \n",
      " mean_loss: 0.18593818\n",
      "epochs: 9 batchs: 4000 \n",
      " mean_loss: 0.18773863\n",
      "epochs: 9 batchs: 5000 \n",
      " mean_loss: 0.18935783\n",
      "epochs: 9 batchs: 6000 \n",
      " mean_loss: 0.19073859\n",
      "epochs: 9 batchs: 7000 \n",
      " mean_loss: 0.19180205\n",
      "epochs: 9 batchs: 8000 \n",
      " mean_loss: 0.19266301\n",
      "epochs: 9 batchs: 9000 \n",
      " mean_loss: 0.19374803\n",
      "epochs: 9 batchs: 10000 \n",
      " mean_loss: 0.19471754\n",
      "epochs: 9 batchs: 11000 \n",
      " mean_loss: 0.19567364\n",
      "epochs: 9 batchs: 12000 \n",
      " mean_loss: 0.19632834\n",
      "epochs: 9 batchs: 13000 \n",
      " mean_loss: 0.19720328\n",
      "epochs: 9 batchs: 14000 \n",
      " mean_loss: 0.1980956\n",
      "epochs: 9 batchs: 15000 \n",
      " mean_loss: 0.1987868\n",
      "epochs: 9 batchs: 16000 \n",
      " mean_loss: 0.19938582\n",
      "epochs: 9 batchs: 17000 \n",
      " mean_loss: 0.20004493\n",
      "epochs: 9 batchs: 18000 \n",
      " mean_loss: 0.20064782\n",
      "eopch 9 - HR = 0.6520, NDCG = 0.3809 elapsed time: 74.32\n",
      "max patience, training stop\n",
      "max hit ratio: 0.6740066225165563 max ndcg ratio: 0.4006229406451058\n"
     ]
    }
   ],
   "source": [
    "## Train model\n",
    "### early stopping을 위한 값 세팅\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    ### data shuffle\n",
    "    user_input, item_input, labels = shuffle(\n",
    "        user_input, item_input, labels\n",
    "    )\n",
    "    ### training\n",
    "    batch_len = len(user_input) // batch_size\n",
    "    batch_loss = list()\n",
    "    for i in range(batch_len):\n",
    "        start = i*batch_size\n",
    "        user_input_bc = user_input[start : start+batch_size]\n",
    "        item_input_bc = item_input[start : start+batch_size]\n",
    "        labels_bc = labels[start : start+batch_size]\n",
    "    \n",
    "        _, l = sess.run([train_op, loss], feed_dict={\n",
    "            user_input_ph: user_input_bc,\n",
    "            item_input_ph: item_input_bc,\n",
    "            labels_ph: labels_bc\n",
    "        })\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        if i % 1000==0:\n",
    "            print(\"epochs:\", epoch, \"batchs:\", i, \"\\n\", \"mean_loss:\", \n",
    "                  np.array(batch_loss).mean())\n",
    "    \n",
    "    ### evaluation\n",
    "    hits, ndcgs = evaluation(sess, testRatings, testNegatives, topK)\n",
    "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print('eopch', epoch, '- HR = %.4f, NDCG = %.4f' % (hr, ndcg), \n",
    "          'elapsed time: %.2f' % (time()-t1))\n",
    "    ### HitRatio가 이전 값보다 작아지면 종료한다.\n",
    "    if hr > best_hr:\n",
    "        best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience == max_patience:\n",
    "            print(\"max patience, training stop\")\n",
    "            print(\"max hit ratio:\", best_hr, \"max ndcg ratio:\", best_ndcg)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
